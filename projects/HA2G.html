<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Generating speech-consistent body and gesture movements is a long-standing problem in virtual avatar creation. 
Previous studies often synthesize pose movement in a holistic manner, where poses of all joints are generated simultaneously. Such a straightforward pipeline fails to generate fine-grained co-speech gestures. 
One observation is that the hierarchical semantics in speech and the hierarchical structures of human gestures can be naturally described into multiple granularities and associated together. 
To fully utilize the rich connections between speech audio and human gestures, we propose a novel framework named Hierarchical Audio-to-Gesture (HA2G) for co-speech gesture generation. 
In HA2G, a Hierarchical Audio Learner extracts audio representations across semantic granularities. A Hierarchical Pose Inferer subsequently renders the entire human pose gradually in a hierarchical manner. 
To enhance the quality of synthesized gestures, we develop a contrastive learning strategy based on audio-text alignment for better audio representations. 
Extensive experiments and human evaluation demonstrate that the proposed method renders realistic co-speech gestures and outperforms previous methods in a clear margin.">
<meta name="keywords" content="Talking Pose Generation; Audio-Visual Representation Learning; Deep learning;">
<link rel="author" href="https://alvinliu0.github.io/">

<!-- Fonts and stuff -->
<link href="./HA2G/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./HA2G/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./HA2G/iconize.css">
<script async="" src="./HA2G/prettify.js"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">

      <div class="section head">
	<h1>Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation</h1>

	<div class="authors">
      Xian Liu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Qianyi Wu</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      Hang Zhou</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Yinghao Xu<sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Rui Qian<sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Xinyi Lin<sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Xiaowei Zhou<sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Wayne Wu</a><sup>4</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Bo Dai</a><sup>5</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  Bolei Zhou</a><sup>1</sup>
	</div>

	<div class="affiliations">

  1. <a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Laboratory, </a> <a href="http://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  2. <a href="https://www.monash.edu/">Monash University</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  3. <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\
  4. <a href="https://www.sensetime.com/en">SenseTime Research </a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  5. <a href="https://www.ntu.edu.sg/">S-Lab, Nanyang Technological University </a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
  </div>

	<div class="venue">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2022. </div>
      </div>

      <center><img src="./HA2G/HA2G.png" border="0" width="90%"></center>

<div class="section abstract">
	<h2>Abstract</h2>
	<br>
	<p>
		Generating speech-consistent body and gesture movements is a long-standing problem in virtual avatar creation. 
Previous studies often synthesize pose movement in a holistic manner, where poses of all joints are generated simultaneously. Such a straightforward pipeline fails to generate fine-grained co-speech gestures. 
One observation is that the hierarchical semantics in speech and the hierarchical structures of human gestures can be naturally described into multiple granularities and associated together. 
To fully utilize the rich connections between speech audio and human gestures, we propose a novel framework named Hierarchical Audio-to-Gesture (HA2G) for co-speech gesture generation. 
In HA2G, a Hierarchical Audio Learner extracts audio representations across semantic granularities. A Hierarchical Pose Inferer subsequently renders the entire human pose gradually in a hierarchical manner. 
To enhance the quality of synthesized gestures, we develop a contrastive learning strategy based on audio-text alignment for better audio representations. 
Extensive experiments and human evaluation demonstrate that the proposed method renders realistic co-speech gestures and outperforms previous methods in a clear margin.
	</p>
      </div>

<div class="section demo">
	<h2>Demo Video</h2>
	<br>
	<center>
	  <iframe width="640" height="360" src="https://www.youtube.com/embed/M7BvPzIBgfM" frameborder="0" allowfullscreen></iframe>
	</video>
	    </center>
	    </div>

<br>

<div class="section materials">
	<h2>Materials</h2>
	<center>
	  <ul>

          <li class="grid">
	      <div class="griditem">
		<a href="https://arxiv.org/abs/2201.07786" target="_blank" class="imageLink"><img src="./HA2G/paper.png"></a><br>
		  <a href="https://arxiv.org/abs/2201.07786" target="_blank">Paper</a>
		</div>
	      </li>

	    </ul>
	    </center>
	    </div>

<br>

<div class="section code">
	<h2>Code</h2>
	<center>
	  <ul>

          <li class="grid">
	      <div class="griditem">
		<img src="./HA2G/code.png"></a><br>
		  <a href="https://github.com/alvinliu0/HA2G" target="_blank">Code [Coming Soon]</a>
		</div>
	      </li>

	    </ul>
	    </center>
	    </div>

<br>


<br>

<div class="section citation">
	<h2>Citation</h2>
	<div class="section bibtex">
	  <pre>
		@inproceedings{liu2022learning,
			title={Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation},
			author={Liu, Xian and Wu, Qianyi and Zhou, Hang and Xu, Yinghao and Qian, Rui and Lin, Xinyi and Zhou, Xiaowei and Wu, Wayne and Dai, Bo and Zhou, Bolei},
			booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
			year={2022}
		  }</pre>
<br>

	  </div>
      </div>

</body></html>
