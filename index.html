<html>
    <head>
        <link rel="stylesheet" type="text/css" href="assets/font-awesome-4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" type="text/css" href="assets/academicons-1.8.6/css/academicons.min.css">
        <!-- <link rel="stylesheet" type="text/css" href="assets/bootstrap-4.3.1-dist/css/bootstrap.min.css"> -->
        <link rel="stylesheet" type="text/css" href="assets/style.css">
        <link rel="icon" type="image/png" href="assets/figures/icon.png">
        <link rel="apple-touch-icon" type="image/png" href="assets/figures/large-icon.png">
        <!-- <link rel="stylesheet" href="font.css"> -->
        <title>Xian Liu's Homepage </title>
    </head>
    
    <body><table border=0 width=1000px align=center><tr><td>
    
        <td valign="top">

        <br>
        <table style="font-size: 11pt;" border=0 width=100%>
            <tr>
                <td> 
                      <p style="text-align:center;font-size: 25pt;">
                        <name>Xian Liu</name>
                        <!-- <br><br><br> -->
                      </p>
                    <font face="helvetica, ariel, 'sans serif'" size="4"> 
                        Member of Technical Staff<br>
                        xAI<br>
                        Seattle, WA<br><br>
                        <a style="font-size: 13pt" href="mailto:alvinliu0430@gmail.com">E-mail</a> &nbsp/&nbsp
                        <a style="font-size: 13pt" href="https://alvinliu0.github.io/misc/CV_Xian_Liu_Public_Version.pdf">CV</a> &nbsp/&nbsp
                        <a style="font-size: 13pt" href="https://scholar.google.com/citations?hl=en&user=QHx5ncgAAAAJ">Google Scholar</a> &nbsp/&nbsp
                        <a style="font-size: 13pt" href="https://github.com/alvinliu0"> Github </a> &nbsp/&nbsp
                        <a style="font-size: 13pt" href="https://twitter.com/AlvinLiu27"> Twitter </a> &nbsp/&nbsp
                        <a style="font-size: 13pt" href="https://www.linkedin.com/in/xian-liu-9840b52a3/"> LinkedIn </a>
                    </font>
                </td>
                <td width = "30%">
                    <img width=250 src="images/me.jpg" border="0">
                        </td>
            </tr>
        </table> 

        
        <!-- <font face="helvetica, ariel, 'sans serif'">  -->
        <h2>Biography</h2>
        <p style="line-height:15pt; font-size: 13pt; text-align: justify; margin:10pt 0px">
        I am a Member of Technical Staff at <a style="font-size: 13pt" href="https://x.ai/">xAI</a>, <a style="font-size: 13pt" href="https://grok.com/imagine">Grok Imagine</a> Team, working on the Omni-model Pretraining and Visual Generation. Before that, I was a Research Scientist at <a style="font-size: 13pt" href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>, <a style="font-size: 13pt" href="https://www.nvidia.com/en-us/ai/cosmos/">Cosmos</a> Team. I obtained Bachelor's degree at Zhejiang University in 2021, and Ph.D. degree at The Chinese University of Hong Kong (CUHK), Multi-Media Laboratory (MMLab) in 2025.
        <br>
        <br>
        My research interest lies in Multi-Modal AI and Foundation Generative Models, with focus on omni-model pre-training and post-training. I am the core contributor to <a style="font-size: 13pt" href="https://grok.com/imagine">xAI Grok Imagine</a>, a leading image and video generation model. I am also core contributor to <a style="font-size: 13pt" href="https://www.nvidia.com/en-us/ai/cosmos/">NVIDIA Cosmos</a> series<a href="https://github.com/NVIDIA/Cosmos">
          <img src="https://img.shields.io/github/stars/NVIDIA/Cosmos.svg?style=social" alt="GitHub stars">
        </a>, an ensemble of open-source world foundation models and their post-trained variants.
        <br>
        </p>
        <!-- </font>        -->
        
        <h2>News</h2>
        <ul id="news-list" style="font-size: 14pt; text-align: justify; margin:10pt 0px;">
            <!-- Initial visible items -->
            <li><strong>[02/2026]</strong> <font color="red" size="3"> <a style="font-size: 12pt" href="https://x.ai/news/grok-imagine-api">Grok Imagine v1.0</a> is released on <a style="font-size: 12pt" href="https://arena.ai">LMArena</a>, with image ranking <a style="font-size: 12pt" href="https://x.com/arena/status/2020184563855815135">Top-3</a> and video ranking <a style="font-size: 12pt" href="https://x.com/arena/status/2019204821551837665">Top-1</a>! </font> </li>
            <li><strong>[01/2026]</strong> <font color="red" size="3"> <a style="font-size: 12pt" href="https://x.ai/news/grok-imagine-api">Grok Imagine v1.0</a> Image and video generation released! </font> </li>
            <li><strong>[12/2025]</strong> <font color="red" size="3"> Join xAI as Member of Technical Staff, working on Omni and Visual Generation models. Stay tuned for our release! </font> </li>
            <!-- Initially hidden items -->
            <div id="more-news" style="display: none;">
            <li><strong>[11/2025]</strong> <font color="black" size="3"> We release <a style="font-size: 12pt" href="https://www.nvidia.com/en-us/ai/cosmos/">Cosmos 2.5</a>, an improved version of world foundation model. Check our <a style="font-size: 12pt" href="https://github.com/nvidia-cosmos/cosmos-predict2.5">Github</a> and <a style="font-size: 12pt" href="https://arxiv.org/abs/2511.00062">technical report</a>. </font> </li>
            <li><strong>[09/2025]</strong> <font color="black" size="3"> One paper is accepted to NeurIPS 2025. </font> </li>
            <li><strong>[07/2025]</strong> <font color="black" size="3"> One paper is accepted to ICCV 2025 with oral presentation. </font> </li>
            <li><strong>[06/2025]</strong> <font color="black" size="3"> Pass my Ph.D. defense and become Dr. Liu officially! </font> </li>
            <li><strong>[06/2025]</strong> <font color="black" size="3"> We release <a style="font-size: 12pt" href="https://developer.nvidia.com/blog/develop-custom-physical-ai-foundation-models-with-nvidia-cosmos-predict-2/">Cosmos-Predict2</a>, a world foundation model with improved quality. Models open-sourced on <a style="font-size: 12pt" href="https://github.com/nvidia-cosmos/cosmos-predict2">Github</a> and <a style="font-size: 12pt" href="https://huggingface.co/collections/nvidia/cosmos-predict2-68028efc052239369a0f2959">HF</a>. </font> </li>
            <li><strong>[03/2025]</strong> <font color="black" size="3"> We release <a style="font-size: 12pt" href="https://research.nvidia.com/labs/dir/cosmos-transfer1/">Cosmos-Transfer1</a>, a world model with multi-modal controllability. Models open-sourced on <a style="font-size: 12pt" href="https://github.com/nvidia-cosmos/cosmos-transfer1">Github</a> and <a style="font-size: 12pt" href="https://huggingface.co/collections/nvidia/cosmos-transfer1-67c9d328196453be6e568d3e">HF</a>. </font> </li>
            <li><strong>[02/2025]</strong> <font color="black" size="3"> Two papers are accepted to CVPR 2025. </font> </li>
            <li><strong>[01/2025]</strong> <font color="black" size="3"> Cosmos won the <a style="font-size: 12pt" href="https://www.cnet.com/tech/these-are-the-official-2025-best-of-ces-winners-awarded-by-cnet-group/">Best of CES, Best of AI, and Best Overall</a> Awards in CNET 2025! </font> </li>
            <li><strong>[01/2025]</strong> <font color="black" size="3"> We release <a style="font-size: 12pt" href="https://www.nvidia.com/en-us/ai/cosmos/">Cosmos</a>, a world foundation model platform for Physical AI. Models open-sourced on <a style="font-size: 12pt" href="https://github.com/NVIDIA/Cosmos">Github</a> and <a style="font-size: 12pt" href="https://huggingface.co/collections/nvidia/cosmos-6751e884dc10e013a0a0d8e6">HF</a>. </font> </li>
            <li><strong>[01/2025]</strong> <font color="black" size="3"> Four papers are accepted to ICLR 2025. </font> </li>
            <li><strong>[12/2024]</strong> <font color="black" size="3"> One paper is accepted to AAAI 2025. </font> </li>
            <li><strong>[11/2024]</strong> <font color="black" size="3"> We release <a style="font-size: 12pt" href="https://research.nvidia.com/labs/dir/cosmos-tokenizer">Cosmos-Tokenizer</a>, a suite of SOTA image/video tokenizers with models available on <a style="font-size: 12pt" href="https://github.com/NVIDIA/Cosmos-Tokenizer">Github</a> and <a style="font-size: 12pt" href="https://huggingface.co/collections/nvidia/cosmos-tokenizer-672b93023add81b66a8ff8e6">HF</a>. </font> </li>
            <li><strong>[09/2024]</strong> <font color="black" size="3"> Honored to receive ECCV 2024 Outstanding Reviewer <a style="font-size: 12pt" href="https://eccv.ecva.net/Conferences/2024/Reviewers">Award</a>. Great thanks for the recognition! </font> </li>
            <li><strong>[07/2024]</strong> <font color="black" size="3"> Two papers are accepted to ECCV 2024. </font> </li>
            <li><strong>[06/2024]</strong> <font color="black" size="3"> Join NVIDIA Research as full-time research scientist, building large-scale foundation models. Stay tuned for our release! </font> </li>
            <!-- <li><strong>[06/2024]</strong> <font color="black" size="3"> I will attend CVPR'24 to present <a style="font-size: 12pt" href="https://alvinliu0.github.io/projects/HumanGaussian">HumanGaussian</a> (Arch 4A-E Poster #180, Wed 19 Jun 5pm—6:30pm) and <a style="font-size: 12pt" href="https://snap-research.github.io/textcraftor/">TextCraftor</a>. Super excited as it's my first on-site conference! Please don't hesitate to drop by our posters and reach out for a casual chat:) </font> </li> -->
            <li><strong>[05/2024]</strong> <font color="black" size="3"> One paper is accepted to ICML 2024. </font> </li>
            <li><strong>[03/2024]</strong> <font color="black" size="3"> Start my internship at NVIDIA Research. See you in Santa Clara! </font> </li>
            <li><strong>[03/2024]</strong> <font color="black" size="3"> Two papers are accepted to CVPR 2024, with HumanGaussian accepted as <font color="red">Highlight (Top 2.8%).</font> See you in Seattle! </font> </li>
            <li><strong>[01/2024]</strong> <font color="black" size="3"> One paper is accepted to ICLR 2024, with HyperHuman receiving review score of 6, 6, 8, 10 <font color="red">(Top 1.6%, <a style="font-size: 12pt" href="https://papercopilot.com/statistics/iclr-statistics/iclr-2024-statistics/">Rank</a>).</font> </font> </li>
            <del><li><strong>[01/2024]</strong> <font color="black" size="3"> I will intern at <a style="font-size: 12pt" href="https://ai.meta.com/genai/">GenAI Team @ Meta AI Research</a> in 2024 Fall. See you in Menlo Park!</font> </li></del>
            <li><strong>[11/2023]</strong> <font color="black" size="3"> I will intern at <a style="font-size: 12pt" href="https://research.nvidia.com/labs/dir/">Deep Imagination Research @ NVIDIA Research</a> in 2024 Spring with <a style="font-size: 12pt" href="https://mingyuliu.net/">Ming-Yu Liu</a>. See you in Santa Clara!</font> </li>
            <li><strong>[11/2023]</strong> <font color="black" size="3"> A high-quality 3D human generation framework <a style="font-size: 12pt" href="https://alvinliu0.github.io/projects/HumanGaussian">HumanGaussian</a> is released, with all the code and models available!</font> </li>
            <li><strong>[10/2023]</strong> <font color="black" size="3"> A hyper-realistic human generation foundation model <a style="font-size: 12pt" href="https://snap-research.github.io/HyperHuman/">HyperHuman</a> collaborated with Snap Research is on arXiv!</font> </li>
            <li><strong>[07/2023]</strong> <font color="black" size="3"> One paper is accepted to ICCV 2023. </font> </li>
            <li><strong>[05/2023]</strong> <font color="black" size="3"> Start my internship at Snap Research. See you in Los Angeles! </font> </li>
            <li><strong>[03/2023]</strong> <font color="black" size="3"> Two papers are accepted to CVPR 2023. </font> </li>
            <li><strong>[03/2023]</strong> <font color="black" size="3"> One paper is accepted to TMLR 2023. </font> </li>
            <li><strong>[09/2022]</strong> <font color="black" size="3"> One paper is accepted to NeurIPS 2022, with ANGIE accepted as <font color="red">Spotlight (Top 5%)!</font></font> </li>
            <li><strong>[07/2022]</strong> <font color="black" size="3"> Three papers are accepted to ECCV 2022, with SSP-NeRF accepted as <font color="red">Oral (Top 2.7%)!</font></font> </li>
            <li><strong>[03/2022]</strong> <font color="black" size="3"> One paper is accepted to CVPR 2022. </font> </li>
            <li><strong>[12/2021]</strong> <font color="black" size="3"> One paper is accepted to AAAI 2022. </font> </li>
            </div>
        </ul>

        <!-- Show More Hyperlink -->
        <a id="show-more-link" href="javascript:void(0);" onclick="showMore()" style="font-size: 12pt;">[Show more]</a>

        <script>
        function showMore() {
            var moreNews = document.getElementById("more-news");
            var link = document.getElementById("show-more-link");
            
            if (moreNews.style.display === "none") {
                moreNews.style.display = "block"; // Show the hidden news
                link.innerHTML = "[Show less]";     // Change link text
            } else {
                moreNews.style.display = "none";  // Hide the news again
                link.innerHTML = "[Show more]";     // Revert link text
            }
        }
        </script>

        <h2>Product Release</h2>
        <p style="margin:-20pt 0px">
        <table style="border-collapse:separate; border-spacing:0px 35px;" cellspacing="8">
          <tbody>

            <tr>
              <td width="23%">
                <img width="200" height="100" style="object-fit: contain; padding: 0px 10px 0px 10px; margin-right: 25px;" src="./images/grok_logo.jpeg">
              </td>
              <td>
                <div class="title">
                  Grok Imagine 1.0
                </div>
                <div class="author">
                  <a style="font-size: 12pt" href="https://grok.com/imagine">xAI Grok Imagine Team</a>: <span class="me"><u>Xian Liu</u> (Core Contributor)</span>.
                </div>
                <div class="conf">
                  Contributions: Grok Imagine Image.
                </div>
                <div>
                  <span class="tag"> <a href="https://x.ai/news/grok-imagine-api" target="_blank">Blogpost</a> </span> /
                  <span class="tag"> <a href="https://x.com/arena/status/2020184563855815135" target="_blank">Image Arena Top-3</a> </span> /
                  <span class="tag"> <a href="https://x.com/arena/status/2019204821551837665" target="_blank">Video Arena Top-1</a> </span> /
                  <span class="tag"> <a href="https://docs.x.ai/developers/model-capabilities/images/generation" target="_blank">Image API Doc</a> </span> /
                  <span class="tag"> <a href="https://docs.x.ai/developers/model-capabilities/video/generation" target="_blank">Video API Doc</a> </span>
                </div>
              </td>
            </tr>

          </tbody>        
          </table>
        </p>

        <h2>Industrial Research</h2>
        <p style="margin:-20pt 0px">
        <table style="border-collapse:separate; border-spacing:0px 35px;" cellspacing="8">
          <tbody>

            <tr>
              <td width="23%">
                <img width="200" height="100" style="object-fit: contain; padding: 0px 10px 0px 10px; margin-right: 25px;" src="./images/cosmos1.gif">
              </td>
              <td>
                <div class="title">
                  Cosmos: World Foundation Model Platform for Physical AI
                </div>
                <div class="author">
                  <a style="font-size: 12pt" href="https://www.nvidia.com/en-us/research/">NVIDIA Cosmos Team</a>: <span class="me"><u>Xian Liu</u> (Core Contributor)</span>.
                </div>
                <div class="conf">
                  Contributions: Auto-Regressive Foundation Model Pre-Training & Post-Training. <font color="red"> (CES'25 Best of AI, Best Overall)</font>
                </div>
                <div>
                  <span class="tag"> <a href="https://www.nvidia.com/en-us/ai/cosmos/" target="_blank">Webpage</a> </span> /
                  <span class="tag"> <a href="https://research.nvidia.com/labs/dir/cosmos1/" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2501.03575" target="_blank">Technical Report</a> </span> /
                  <span class="tag"> <a href="https://blogs.nvidia.com/blog/cosmos-world-foundation-models/" target="_blank">Blog</a> </span> /
                  <span class="tag"> <a href="https://github.com/NVIDIA/Cosmos" target="_blank">Github</a> </span> /
                  <span class="tag"> <a href="https://huggingface.co/collections/nvidia/cosmos-6751e884dc10e013a0a0d8e6" target="_blank">HuggingFace</a> </span> /
                  <span class="tag"> <a href="https://www.youtube.com/watch?v=9Uch931cDx8" target="_blank">Demo</a> </span> /
                  <span class="tag"> <a href="https://www.youtube.com/live/k82RwXqZHY8?t=3303s" target="_blank">Keynote (Jensen Huang, CES'25)</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="180" height="100" style="padding: 0px 10px 0px 10px; margin-right: 25px;" src="./images/cosmos-predict2.gif">
              </td>
              <td>
                <div class="title">
                  Cosmos 2.5: Improved World Simulation with Video Foundation Models for Physical AI
                </div>
                <div class="author">
                  <a style="font-size: 12pt" href="https://www.nvidia.com/en-us/research/">NVIDIA Cosmos Team</a>: <span class="me"><u>Xian Liu</u> (Core Contributor)</span>.
                </div>
                <div class="conf">
                  Contributions: Data Processing Pipelines, Captioning, Long Video Generation, Evaluation, Transfer Post-training.
                </div>
                <div>
                  <span class="tag"> <a href="https://www.nvidia.com/en-us/ai/cosmos/" target="_blank">Webpage</a> </span> /
                  <span class="tag"> <a href="https://research.nvidia.com/labs/dir/cosmos-predict2.5/" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2511.00062" target="_blank">Technical Report</a> </span> /
                  <span class="tag"> <a href="https://developer.nvidia.com/blog/develop-custom-physical-ai-foundation-models-with-nvidia-cosmos-predict-2/" target="_blank">Blog</a> </span> /
                  <span class="tag"> <a href="https://github.com/nvidia-cosmos/cosmos-predict2.5" target="_blank">Github</a> </span> /
                  <span class="tag"> <a href="https://huggingface.co/collections/nvidia/cosmos-predict25-68bb63255f2fc206c5e5b346" target="_blank">HuggingFace</a> </span> /
                  <span class="tag"> <a href="https://research.nvidia.com/labs/dir/pbench/" target="_blank">Benchmark</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="80" style="padding: 10px 0px 10px 0px; margin-right: 25px;" src="./images/tokenizer.jpg">
              </td>
              <td>
                <div class="title">
                  Cosmos Tokenizer: A Suite of Image and Video Neural Tokenizers
                </div>
                <div class="author">
                  <a style="font-size: 12pt" href="https://www.nvidia.com/en-us/research/">NVIDIA Cosmos Team</a>: <span class="me"><u>Xian Liu</u> (Core Contributor)</span>.
                </div>
                <div class="conf">
                  Contributions: Continuous/Discrete Image/Video Tokenizers.
                </div>
                <div>
                  <span class="tag"> <a href="https://research.nvidia.com/labs/dir/cosmos-tokenizer" target="_blank">Webpage</a> </span> /
                  <span class="tag"> <a href="https://research.nvidia.com/labs/dir/cosmos-tokenizer" target="_blank">Technical Report</a> </span> /
                  <span class="tag"> <a href="https://developer.nvidia.com/blog/state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo/" target="_blank">Blog</a> </span> /
                  <span class="tag"> <a href="https://github.com/NVIDIA/Cosmos-Tokenizer" target="_blank">Github</a> </span> /
                  <span class="tag"> <a href="https://huggingface.co/collections/nvidia/cosmos-tokenizer-672b93023add81b66a8ff8e6" target="_blank">HuggingFace</a> </span> /
                  <span class="tag"> <a href="https://youtu.be/Soy_myOfWIU" target="_blank">Demo</a> </span> /
                  <span class="tag"> <a href="https://github.com/NVlabs/TokenBench" target="_blank">Benchmark</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="180" height="100" style="padding: 0px 10px 0px 10px; margin-right: 25px;" src="./images/cosmos-transfer1.gif">
              </td>
              <td>
                <div class="title">
                  Cosmos-Transfer: World Generation with Adaptive Multimodal Control
                </div>
                <div class="author">
                  <a style="font-size: 12pt" href="https://www.nvidia.com/en-us/research/">NVIDIA Cosmos Team</a>: <span class="me"><u>Xian Liu</u> (Core Contributor)</span>.
                </div>
                <div class="conf">
                  Contributions: Adaptive Multi-Modal Control, Data Processing Pipelines, Open-Source Repo.
                </div>
                <div>
                  <span class="tag"> <a href="https://www.nvidia.com/en-us/ai/cosmos/" target="_blank">Webpage</a> </span> /
                  <span class="tag"> <a href="https://research.nvidia.com/labs/dir/cosmos-transfer1/" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2503.14492" target="_blank">Technical Report</a> </span> /
                  <span class="tag"> <a href="https://www.youtube.com/watch?v=0Yr5SdrVnxc" target="_blank">Demo</a> </span> /
                  <span class="tag"> <a href="https://github.com/nvidia-cosmos/cosmos-transfer1" target="_blank">Github</a> </span> /
                  <span class="tag"> <a href="https://huggingface.co/collections/nvidia/cosmos-transfer1-67c9d328196453be6e568d3e" target="_blank">HuggingFace</a> </span>
                </div>
              </td>
            </tr>

          </tbody>        
          </table>
        </p>

        <h2>Selected Publications [<a style="font-size: 15pt" href="https://scholar.google.com/citations?hl=en&user=QHx5ncgAAAAJ"> Full List </a>] <font color="black" size="2">(* indicates equal contribution)</font> </h2>
        <p style="margin:-20pt 0px">
          <table style="border-collapse:separate; border-spacing:0px 35px;" cellspacing="8">
          <tbody>

            <tr>
              <td width="23%">
                <img width="180" height="125" style="padding: 0px 10px 0px 10px" src="./images/humangaussian-teaser.gif">
              </td>
              <td>
                <div class="title">
                  HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting
                </div>
                <div class="author">
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://xiaohangzhan.github.io/">Xiaohang Zhan</a>,
                  <a href="https://me.kiui.moe/">Jiaxiang Tang</a>,
                  <a href='https://scholar.google.com/citations?user=4oXBp9UAAAAJ'>Ying Shan</a>,
                  <a href="https://scholar.google.com/citations?user=RuHyY6gAAAAJ">Gang Zeng</a>,
                  <a href="http://dahua.site/">Dahua Lin</a>,
                  <a href="https://xh-liu.github.io/">Xihui Liu</a>,
                  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>.
                </div>
                <div class="conf">
                  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <font color="red"> (Highlight, Top 2.8%)</font>
                </div>
                <div>
                  <span class="venue"> <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a> </span> /
                  <span class="tag"> <a href="https://alvinliu0.github.io/projects/HumanGaussian" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://alvinliu0.github.io/projects/HumanGaussian/humangaussian.pdf" target="_blank">Paper</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2311.17061" target="_blank">arXiv</a> </span> /
                  <span class="tag"> <a href="https://www.youtube.com/watch?v=S3djzHoqPKY" target="_blank">Demo Video</a> </span> /
                  <span class="tag"> <a href="https://github.com/alvinliu0/HumanGaussian" target="_blank">Code</a> </span> /
                  <span class="tag"> <a href="https://github.com/alvinliu0/HumanGaussian#pretrained-models" target="_blank">Models</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="180" height="125" style="padding: 0px 10px 0px 10px" src="./images/hyperhuman.png">
              </td>
              <td>
                <div class="title">
                  HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion
                </div>
                <div class="author">
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://alanspike.github.io/">Jian Ren</a>,
                  <a href="https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/">Aliaksandr Siarohin</a>,
                  <a href='https://universome.github.io/'>Ivan Skorokhodov</a>,
                  <a href="https://scholar.google.com/citations?user=XUj8koUAAAAJ&hl=en">Yanyu Li</a>,
                  <a href="http://dahua.site/">Dahua Lin</a>,
                  <a href="https://xh-liu.github.io/">Xihui Liu</a>,
                  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>,
                  <a href="http://www.stulyakov.com/">Sergey Tulyakov</a>.
                </div>
                <div class="conf">
                  International Conference on Learning Representations (<b>ICLR</b>), 2024. <font color="red"> (Review Score 6, 6, 8, 10, Top 1.6%, <a style="font-size: 12pt" href="https://papercopilot.com/statistics/iclr-statistics/iclr-2024-statistics/">Rank</a>)</font>
                </div>
                <div>
                  <span class="venue"> <a href="https://iclr.cc/Conferences/2024">ICLR 2024</a> </span> /
                  <span class="tag"> <a href="https://openreview.net/forum?id=duyA42HlCK" target="_blank">OpenReview</a> </span> /
                  <span class="tag"> <a href="https://snap-research.github.io/HyperHuman/" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://snap-research.github.io/HyperHuman/content/hyperhuman.pdf" target="_blank">Paper</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2310.08579" target="_blank">arXiv</a> </span> /
                  <span class="tag"> <a href="https://www.youtube.com/watch?v=eRPZW1pwxog" target="_blank">Short Demo (3min)</a> </span> /
                  <span class="tag"> <a href="https://www.youtube.com/watch?v=CxGfbwZOcyU" target="_blank">Long Demo (10min)</a> </span> /
                  <span class="tag"> <a href="https://github.com/snap-research/HyperHuman" target="_blank">Github</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="100" style="padding: 12.5px 0px 12.5px 0px" src="./images/SSP-NeRF.png">
              </td>
              <td>
                <div class="title">
                  Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation
                </div>
                <div class="author">
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://justimyhxu.github.io/">Yinghao Xu</a>,
                  <a href="https://wuqianyi.top/">Qianyi Wu</a>,
                  <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>,
                  <a href="https://wywu.github.io/">Wayne Wu</a>,
                  <a href="http://bzhou.ie.cuhk.edu.hk/">Bolei Zhou</a>.
                </div>
                <div class="conf">
                  European Conference on Computer Vision (<b>ECCV</b>), 2022. <font color="red"> (Oral, Top 2.7%)</font>
                </div>
                <div>
                  <span class="venue"> <a href="https://eccv2022.ecva.net/">ECCV 2022</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/pdf/2201.07786.pdf" target="_blank">Paper</a> </span> /
                  <span class="tag"> <a href="https://alvinliu0.github.io/projects/SSP-NeRF" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://github.com/alvinliu0/SSP-NeRF" target="_blank">Code</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="100" style="padding: 12.5px 0px 12.5px 0px" src="./images/angie.png">
              </td>
              <td>
                <div class="title">
                  Audio-Driven Co-Speech Gesture Video Generation
                </div>
                <div class="author">
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://wuqianyi.top/">Qianyi Wu</a>,
                  <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>,
                  <a href='https://yuanqidu.github.io/'>Yuanqi Du</a>,
                  <a href="https://wywu.github.io/">Wayne Wu</a>,
                  <a href="http://dahua.site/">Dahua Lin</a>,
                  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>.
                </div>
                <div class="conf">
                  Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2022. <font color="red"> (Spotlight, Top 5%)</font>
                </div>
                <div>
                  <span class="venue"> <a href="https://neurips.cc/">NeurIPS 2022</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/pdf/2212.02350.pdf" target="_blank">Paper</a> </span> /
                  <span class="tag"> <a href="https://alvinliu0.github.io/projects/ANGIE" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://github.com/alvinliu0/ANGIE" target="_blank">Code</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="105" style="padding: 10px 0px 10px 0px" src="./images/hmar.png">
              </td>
              <td>
                <div class="title">
                  HMAR: Efficient Hierarchical Masked AutoRegressive Image Generation
                </div>
                <div class="author">
                  <a href="https://scholar.google.com/citations?user=mieuBzUAAAAJ&hl=en">Hermann Kumbong*</a>,
                  <span class="me"><u>Xian Liu*</u></span>,
                  <a href="https://tsungyilin.info/">Tsung-Yi Lin</a>,
                  <a href="https://xh-liu.github.io/">Xihui Liu</a>,
                  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>,
                  <a href="http://www.danfu.org/">Daniel Y. Fu</a>,
                  <a href="http://mingyuliu.net/">Ming-Yu Liu</a>,
                  <a href="https://cs.stanford.edu/~chrismre/">Christopher Ré</a>,
                  <a href="https://davidwromero.xyz/">David W. Romero</a>.
                </div>
                <div class="conf">
                  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025.
                </div>
                <div>
                  <span class="venue"> <a href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a> </span> /
                  <span class="tag"> <a href="https://research.nvidia.com/labs/dir/hmar/" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2506.04421" target="_blank">Paper</a> </span> /
                  <span class="tag"> <a href="https://huggingface.co/nvidia/HMAR" target="_blank">HuggingFace</a> </span> /
                  <span class="tag"> <a href="https://github.com/NVlabs/HMAR" target="_blank">Github</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="125" src="./images/tc4d.gif">
              </td>
              <td>
                <div class="title">
                  TC4D: Trajectory-Conditioned Text-to-4D Generation
                </div>
                <div class="author">
                  <a href="https://sherwinbahmani.github.io/">Sherwin Bahmani*</a>,
                  <span class="me"><u>Xian Liu*</u></span>,
                  <a href="https://yifita.netlify.app/">Yifan Wang*</a>,
                  <a href="https://universome.github.io/">Ivan Skorokhodov</a>,
                  <a href="https://www.lessvrong.com/">Victor Rong</a>,
                  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>,
                  <a href="https://xh-liu.github.io/">Xihui Liu</a>,
                  <a href="https://jjparkcv.github.io/">Jeong Joon Park</a>,
                  <a href="http://www.stulyakov.com/">Sergey Tulyakov</a>,
                  <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
                  <a href="https://taiya.github.io/">Andrea Tagliasacchi</a>,
                  <a href="https://davidlindell.com/">David B. Lindell</a>.
                </div>
                <div class="conf">
                  European Conference on Computer Vision (<b>ECCV</b>), 2024.
                </div>
                <div>
                  <span class="venue"> <a href="https://eccv2024.ecva.net/">ECCV 2024</a> </span> /
                  <span class="tag"> <a href="https://sherwinbahmani.github.io/tc4d/" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2403.17920" target="_blank">Paper</a> </span> /
                  <span class="tag"> <a href="https://github.com/sherwinbahmani/tc4d" target="_blank">Github</a> </span>
                </div>
              </td>
            </tr>

          </tbody>        
          </table>
        </p>

        <h2>Working Experiences</h2>

        <style>
          .intern-item{
            display:flex;
            align-items:flex-start;
            padding:8px 0;
          }
          .intern-item .img-container{
            width:150px;
            height:90px;
            display:flex;
            align-items:center;
            justify-content:center;
            margin-right:16px;
            flex:0 0 150px;
          }
          .intern-item .img-container img{
            max-width:150px;
            max-height:90px;
            width:auto;
            height:auto;
            object-fit:contain;
          }
          .intern-content .title{ font-weight:600; margin-bottom:4px; }
          .intern-content .author{ color:#444; margin-bottom:6px; }
          .intern-content .conf{ margin-bottom:6px; }
          .intern-content a{ color:inherit; text-decoration:underline; }
        </style>
        
        <p style="margin:-20pt 0px">
          <table style="border-collapse:separate; border-spacing:10px 30px;" cellspacing="8" width="100%">
          <tbody>

            <tr>
              <td width="50%" valign="top">
                <div class="intern-item">
                  <div class="img-container">
                    <img src="./images/xai.png" alt="xAI">
                  </div>
                  <div class="intern-content">
                    <div class="title">Member of Technical Staff.</div>
                    <div class="author">Dec. 2025 - Now</div>
                    <div class="conf">xAI, Omni Pretrain Team.</div>
                  </div>
                </div>
              </td>
              <td width="50%" valign="top">
                <div class="intern-item">
                  <div class="img-container">
                    <img src="./images/nvidia.png" alt="NVIDIA">
                  </div>
                  <div class="intern-content">
                    <div class="title">Research Scientist.</div>
                    <div class="author">Jun. 2024 - Dec. 2025</div>
                    <div class="conf">NVIDIA Research, Cosmos Team.</div>
                  </div>
                </div>
              </td>
            </tr>

          </tbody>        
          </table>
        </p>

        <h2>Internship Experiences</h2>

        <p style="margin:-20pt 0px">
          <table style="border-collapse:separate; border-spacing:10px 30px;" cellspacing="8" width="100%">
          <tbody>

            <tr>
              <td width="50%" valign="top">
                <div class="intern-item">
                  <div class="img-container">
                    <img src="./images/nvidia.png" alt="NVIDIA">
                  </div>
                  <div class="intern-content">
                    <div class="title">Generative AI Research Intern.</div>
                    <div class="author">Mar. 2024 - Jun. 2024</div>
                    <div class="conf">NVIDIA Research, Deep Imagination Group.</div>
                    <div>Supervisor: <a href="https://tcwang0509.github.io/">Ting-Chun Wang</a>, <a href="https://mingyuliu.net">Ming-Yu Liu</a>.</div>
                  </div>
                </div>
              </td>
              <td width="50%" valign="top">
                <div class="intern-item">
                  <div class="img-container">
                    <img src="./images/UofT-logo.svg.png" alt="University of Toronto">
                  </div>
                  <div class="intern-content">
                    <div class="title">Research Visiting Student.</div>
                    <div class="author">Dec. 2023 - Mar. 2024</div>
                    <div class="conf">UofT, Toronto Computational Imaging Group.</div>
                    <div>Host: <a href="https://sherwinbahmani.github.io/">Sherwin Bahmani</a>, <a href="https://davidlindell.com/">David B. Lindell</a>.</div>
                  </div>
                </div>
              </td>
            </tr>

            <tr>
              <td width="50%" valign="top">
                <div class="intern-item">
                  <div class="img-container">
                    <img src="./images/txailab.jpeg" alt="Tencent AI Lab">
                  </div>
                  <div class="intern-content">
                    <div class="title">Research Intern.</div>
                    <div class="author">Sept. 2023 - Dec. 2023</div>
                    <div class="conf">Tencent AI Laboratory.</div>
                    <div>Supervisor: <a href="https://xiaohangzhan.github.io/">Xiaohang Zhan</a>, <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ">Ying Shan</a>.</div>
                  </div>
                </div>
              </td>
              <td width="50%" valign="top">
                <div class="intern-item">
                  <div class="img-container">
                    <img src="./images/snap.png" alt="Snap Research">
                  </div>
                  <div class="intern-content">
                    <div class="title">Research Intern.</div>
                    <div class="author">May. 2023 - Sept. 2023</div>
                    <div class="conf">Snap Research, Creative Vision Group.</div>
                    <div>Supervisor: <a href="https://alanspike.github.io/">Jian Ren</a>, <a href="http://www.stulyakov.com/">Sergey Tulyakov</a>.</div>
                  </div>
                </div>
              </td>
            </tr>

            <tr>
              <td width="50%" valign="top">
                <div class="intern-item">
                  <div class="img-container">
                    <img src="./images/shlab.png" alt="Shanghai AI Lab">
                  </div>
                  <div class="intern-content">
                    <div class="title">Research Intern.</div>
                    <div class="author">Jul. 2021 - Feb. 2022</div>
                    <div class="conf">Shanghai AI Lab, Digital Content Group.</div>
                    <div>Supervisor: <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>, <a href="https://wywu.github.io/">Wayne Wu</a>.</div>
                  </div>
                </div>
              </td>
              <td width="50%" valign="top">
                <div class="intern-item">
                  <div class="img-container">
                    <img src="./images/sensetime.png" alt="SenseTime">
                  </div>
                  <div class="intern-content">
                    <div class="title">Research Intern.</div>
                    <div class="author">Aug. 2020 - Jun. 2021</div>
                    <div class="conf">SenseTime Research, Intelligent Video Group.</div>
                    <div>Supervisor: <a href="https://wuqianyi.top/">Qianyi Wu</a>, <a href="http://daibo.info/">Bo Dai</a>.</div>
                  </div>
                </div>
              </td>
            </tr>

          </tbody>        
          </table>
        </p>

        <h2>Invited Talks</h2>
        <!-- <font face="helvetica, ariel, 'sans serif'">  -->
        <ul style="font-size: 12pt; text-align: justify;">
        <li> The University of Hong Kong: <a href="https://www.hku.hk/">Towards Multi-Modal Visual Generation: From Human Modeling to World Foundation Models</a>. <div style="float:right; text-align:right">2025</div> </li>
        <li> Nanjing University: <a href="https://www.nju.edu.cn/en/">Towards Multi-Modal Visual Generation: From Human Modeling to World Foundation Models</a>. <div style="float:right; text-align:right">2025</div> </li>
        <li> SenseTime: <a href="https://aitime.cn/">Towards Multi-Modal Visual Generation: From Human Modeling to World Foundation Models</a>. <div style="float:right; text-align:right">2025</div> </li>
        <li> AI TIME: <a href="https://aitime.cn/">HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion</a>. <div style="float:right; text-align:right">2024</div> </li>
        <li> AI TIME: <a href="https://aitime.cn/">Audio-Driven Co-Speech Gesture Video Generation</a>. <div style="float:right; text-align:right">2023</div> </li>
        <li> TechBeat: <a href="https://www.techbeat.net/">Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation</a>. <div style="float:right; text-align:right">2022</div> </li>
        </ul>

        <h2>Professional Services</h2>
        <!-- <font face="helvetica, ariel, 'sans serif'">  -->
          <ul style="font-size: 12pt; text-align: justify;">
          <li>Area Chair / Senior Program Committee: AAAI.</li>
          <li>Conference Reviewer: CVPR, ECCV, ICCV, WACV, SIGGRAPH, SIGGRAPH Asia, ACL, NeurIPS, ICML, ICLR, AISTATS, AAAI, ACM MM.</li>
          <li>Journal Reviewer: TPAMI, IJCV, TVCG, TIP, TMM, EG, CGF, PG.</li>
        </ul>
        <!-- </font> -->


        <h2>Selected Honors & Awards</h2>
        <!-- <font face="helvetica, ariel, 'sans serif'">  -->
        <ul style="font-size: 12pt; text-align: justify;">
        <li> CNET 2025 Best of CES, Best of AI, and Best Overall. <div style="float:right; text-align:right">2025</div> </li>
        <li> ECCV Outstanding Reviewer Award. <div style="float:right; text-align:right">2024</div> </li>
        <li> CVPR Travel Award. <div style="float:right; text-align:right">2024</div> </li>
        <li> ICLR Travel Award. <div style="float:right; text-align:right">2024</div> </li>
        <li> National Scholarship. <div style="float:right; text-align:right">2019, 2020</div> </li>
        <li> Hong Kong Ph.D. Fellowship Scheme (HKPFS). <div style="float:right; text-align:right">2021- 2025</div> </li>
        <li> Outstanding Graduate of Zhejiang Province.  <div style="float:right; text-align:right">2021</div> </li>
        <li> Outstanding Bachelor Thesis Award of Zhejiang University, Top 1%.  <div style="float:right; text-align:right">2021</div> </li>
        <li> UCLA CSST Scholarship Program.  <div style="float:right; text-align:right">2020</div> </li>
        <li> SenseTime Scholarship.  <div style="float:right; text-align:right">2020</div> </li>
        <li> Tang Lixin Scholarship.  <div style="float:right; text-align:right">2019</div> </li>
        <li> First Class Scholarship for Academic Excellence.  <div style="float:right; text-align:right">2019, 2020</div> </li>
        </ul>
        <!-- </font> -->

        <h2>Teaching Experience</h2>
        <!-- <font face="helvetica, ariel, 'sans serif'">  -->
          <ul style="font-size: 12pt; text-align: justify;">
          <!-- Teaching Assistant of the following courses in The Chinese University of Hong Kong:  -->
          <li>ENGG 1120, Linear Algebra for Engineers. <div style="float:right; text-align:right">Spring 2022.</div></li>
          <li>ENGG 2440, Discrete Mathematics for Engineers. <div style="float:right; text-align:right">Fall 2021.</div></li>
        </ul>
        <!-- </font> -->
      
        </body>


</html>
