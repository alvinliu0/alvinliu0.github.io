<html>
    <head>
        <link rel="stylesheet" type="text/css" href="assets/font-awesome-4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" type="text/css" href="assets/academicons-1.8.6/css/academicons.min.css">
        <!-- <link rel="stylesheet" type="text/css" href="assets/bootstrap-4.3.1-dist/css/bootstrap.min.css"> -->
        <link rel="stylesheet" type="text/css" href="assets/style.css">
        <link rel="icon" type="image/png" href="assets/figures/icon.png">
        <link rel="apple-touch-icon" type="image/png" href="assets/figures/large-icon.png">
        <!-- <link rel="stylesheet" href="font.css"> -->
        <title>Xian Liu's Homepage </title>
    </head>
    
    <body><table border=0 width=1000px align=center><tr><td>
    
        <td valign="top">

        <br>
        <table style="font-size: 11pt;" border=0 width=100%>
            <tr>
                <td> 
                      <p style="text-align:center;font-size: 25pt;">
                        <name>Xian Liu</name>
                        <!-- <br><br><br> -->
                      </p>
                    <font face="helvetica, ariel, 'sans serif'" size="4"> 
                        Research Scientist<br>
                        NVIDIA Research<br>
                        Santa Clara, CA<br><br>
                        <a style="font-size: 13pt" href="mailto:alvinliu0430@gmail.com">E-mail</a> &nbsp/&nbsp
                        <a style="font-size: 13pt" href="https://alvinliu0.github.io/misc/CV_Xian_Liu_Public_Version.pdf">CV</a> &nbsp/&nbsp
                        <a style="font-size: 13pt" href="https://scholar.google.com/citations?hl=en&user=QHx5ncgAAAAJ">Google Scholar</a> &nbsp/&nbsp
                        <a style="font-size: 13pt" href="https://github.com/alvinliu0"> Github </a> &nbsp/&nbsp
                        <a style="font-size: 13pt" href="https://twitter.com/AlvinLiu27"> Twitter </a> &nbsp/&nbsp
                        <a style="font-size: 13pt" href="https://www.linkedin.com/in/xian-liu-9840b52a3/"> LinkedIn </a>
                    </font>
                </td>
                <td width = "30%">
                    <img width=250 src="images/me.jpg" border="0">
                        </td>
            </tr>
        </table> 

        
        <!-- <font face="helvetica, ariel, 'sans serif'">  -->
        <h2>Biography</h2>
        <p style="line-height:15pt; font-size: 13pt; text-align: justify; margin:10pt 0px">
        I am a Research Scientist at <a style="font-size: 12pt" href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>, <a style="font-size: 12pt" href="https://research.nvidia.com/labs/dir/">Deep Imagination Research Group</a>. I am also a final-year Ph.D. at <a style="font-size: 12pt" href="http://mmlab.ie.cuhk.edu.hk/">CUHK Multi-Media Lab (MMLab)</a>, supervised by <a style="font-size: 12pt" href="http://dahua.site/"> Prof. Dahua Lin</a>, <a style="font-size: 12pt" href="https://liuziwei7.github.io/"> Prof. Ziwei Liu</a>, and <a style="font-size: 12pt" href="https://xh-liu.github.io/"> Prof. Xihui Liu</a>. 
        Before that, I received the Bachelor's degree at Zhejiang University in 2021, advised by <a style="font-size: 12pt" href="https://xzhou.me/">Prof. Xiaowei Zhou</a>.
        <br>
        <br>
        I am fortunate to have extensive industrial experience during Ph.D. study, with multiple internships at several leading research institutes, including <a style="font-size: 12pt" href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>, <a style="font-size: 12pt" href="https://research.snap.com/">Snap Research</a>, <a style="font-size: 12pt" href="https://ai.tencent.com/ailab/en/index">Tencent AI Lab</a>, <a style="font-size: 12pt" href="https://research.snap.com/">SenseTime Research</a>, and <a style="font-size: 12pt" href="https://www.shlab.org.cn/">Shanghai AI Lab</a>.
        <br>
        <br>
        My research interests include computer vision and generative modeling, especially the foundation GenAI pre-training / post-training, vision-language models, multi-modal tokenizers, and their applications in digital humans and physical AI.
        <br>
        <br>
        <strong> <font color="red" size="3"> I am always open to discussions and collaborations, feel free to drop me an email if you are interested in :) </font></strong>
        <br>
        </p>
        <!-- </font>        -->
        
        <h2>News</h2>
        <ul id="news-list" style="font-size: 14pt; text-align: justify; margin:10pt 0px;">
            <!-- Initial visible items -->
            <li><strong>[02/2025]</strong> <font color="red" size="3"> Two papers are accepted to CVPR 2025. </font> </li>
            <li><strong>[01/2025]</strong> <font color="red" size="3"> Cosmos won the <a style="font-size: 12pt" href="https://www.cnet.com/tech/these-are-the-official-2025-best-of-ces-winners-awarded-by-cnet-group/">Best of CES, Best of AI, and Best Overall</a> Awards in CNET 2025! </font> </li>
            <li><strong>[01/2025]</strong> <font color="red" size="3"> We release <a style="font-size: 12pt" href="https://www.nvidia.com/en-us/ai/cosmos/">Cosmos</a>, a world foundation model platform for Physical AI. Models open-sourced on <a style="font-size: 12pt" href="https://github.com/NVIDIA/Cosmos">Github</a> and <a style="font-size: 12pt" href="https://huggingface.co/collections/nvidia/cosmos-6751e884dc10e013a0a0d8e6">HF</a>! </font> </li>
            <li><strong>[01/2025]</strong> <font color="red" size="3"> Four papers are accepted to ICLR 2025. </font> </li>
            <li><strong>[12/2024]</strong> <font color="black" size="3"> One paper is accepted to AAAI 2025. </font> </li>
            <li><strong>[11/2024]</strong> <font color="black" size="3"> We release <a style="font-size: 12pt" href="https://research.nvidia.com/labs/dir/cosmos-tokenizer">Cosmos-Tokenizer</a>, a suite of SOTA image/video tokenizers with models available on <a style="font-size: 12pt" href="https://github.com/NVIDIA/Cosmos-Tokenizer">Github</a> and <a style="font-size: 12pt" href="https://huggingface.co/collections/nvidia/cosmos-tokenizer-672b93023add81b66a8ff8e6">HF</a>! </font> </li>
            <li><strong>[09/2024]</strong> <font color="black" size="3"> Honored to receive ECCV 2024 Outstanding Reviewer <a style="font-size: 12pt" href="https://eccv.ecva.net/Conferences/2024/Reviewers">Award</a>. Great thanks for the recognition! </font> </li>
            <!-- Initially hidden items -->
            <div id="more-news" style="display: none;">
            <li><strong>[07/2024]</strong> <font color="black" size="3"> Two papers are accepted to ECCV 2024. </font> </li>
            <!-- <li><strong>[06/2024]</strong> <font color="black" size="3"> I will attend CVPR'24 to present <a style="font-size: 12pt" href="https://alvinliu0.github.io/projects/HumanGaussian">HumanGaussian</a> (Arch 4A-E Poster #180, Wed 19 Jun 5pmâ€”6:30pm) and <a style="font-size: 12pt" href="https://snap-research.github.io/textcraftor/">TextCraftor</a>. Super excited as it's my first on-site conference! Please don't hesitate to drop by our posters and reach out for a casual chat:) </font> </li> -->
            <li><strong>[05/2024]</strong> <font color="black" size="3"> One paper is accepted to ICML 2024. </font> </li>
            <li><strong>[03/2024]</strong> <font color="black" size="3"> Start my internship at NVIDIA Research. See you in Santa Clara! </font> </li>
            <li><strong>[03/2024]</strong> <font color="black" size="3"> Two papers are accepted to CVPR 2024, with HumanGaussian accepted as <font color="red">Highlight (Top 2.8%).</font> See you in Seattle! </font> </li>
            <li><strong>[01/2024]</strong> <font color="black" size="3"> One paper is accepted to ICLR 2024, with HyperHuman receiving review score of 6, 6, 8, 10 <font color="red">(Top 1.6%, <a style="font-size: 12pt" href="https://papercopilot.com/statistics/iclr-statistics/iclr-2024-statistics/">Rank</a>).</font> </font> </li>
            <del><li><strong>[01/2024]</strong> <font color="black" size="3"> I will intern at <a style="font-size: 12pt" href="https://ai.meta.com/genai/">GenAI Team @ Meta AI Research</a> in 2024 Fall. See you in Menlo Park!</font> </li></del>
            <li><strong>[11/2023]</strong> <font color="black" size="3"> I will intern at <a style="font-size: 12pt" href="https://research.nvidia.com/labs/dir/">Deep Imagination Research @ NVIDIA Research</a> in 2024 Spring with <a style="font-size: 12pt" href="https://mingyuliu.net/">Ming-Yu Liu</a>. See you in Santa Clara!</font> </li>
            <li><strong>[11/2023]</strong> <font color="black" size="3"> A high-quality 3D human generation framework <a style="font-size: 12pt" href="https://alvinliu0.github.io/projects/HumanGaussian">HumanGaussian</a> is released, with all the code and models available!</font> </li>
            <li><strong>[10/2023]</strong> <font color="black" size="3"> A hyper-realistic human generation foundation model <a style="font-size: 12pt" href="https://snap-research.github.io/HyperHuman/">HyperHuman</a> collaborated with Snap Research is on arXiv!</font> </li>
            <li><strong>[07/2023]</strong> <font color="black" size="3"> One paper is accepted to ICCV 2023. </font> </li>
            <li><strong>[05/2023]</strong> <font color="black" size="3"> Start my internship at Snap Research. See you in Los Angeles! </font> </li>
            <li><strong>[03/2023]</strong> <font color="black" size="3"> Two papers are accepted to CVPR 2023. </font> </li>
            <li><strong>[03/2023]</strong> <font color="black" size="3"> One paper is accepted to TMLR 2023. </font> </li>
            <li><strong>[09/2022]</strong> <font color="black" size="3"> One paper is accepted to NeurIPS 2022, with ANGIE accepted as <font color="red">Spotlight (Top 5%)!</font></font> </li>
            <li><strong>[07/2022]</strong> <font color="black" size="3"> Three papers are accepted to ECCV 2022, with SSP-NeRF accepted as <font color="red">Oral (Top 2.7%)!</font></font> </li>
            <li><strong>[03/2022]</strong> <font color="black" size="3"> One paper is accepted to CVPR 2022. </font> </li>
            <li><strong>[12/2021]</strong> <font color="black" size="3"> One paper is accepted to AAAI 2022. </font> </li>
            </div>
        </ul>

        <!-- Show More Hyperlink -->
        <a id="show-more-link" href="javascript:void(0);" onclick="showMore()" style="font-size: 12pt;">[Show more]</a>

        <script>
        function showMore() {
            var moreNews = document.getElementById("more-news");
            var link = document.getElementById("show-more-link");
            
            if (moreNews.style.display === "none") {
                moreNews.style.display = "block"; // Show the hidden news
                link.innerHTML = "[Show less]";     // Change link text
            } else {
                moreNews.style.display = "none";  // Hide the news again
                link.innerHTML = "[Show more]";     // Revert link text
            }
        }
        </script>

        <h2>Industrial Research</h2>
        <p style="margin:-20pt 0px">
        <table style="border-collapse:separate; border-spacing:0px 35px;" cellspacing="8">
          <tbody>

            <tr>
              <td width="23%">
                <img width="180" height="100" style="padding: 0px 10px 0px 10px; margin-right: 25px;" src="./images/cosmos1.gif">
              </td>
              <td>
                <div class="title">
                  Cosmos World Foundation Model Platform for Physical AI
                </div>
                <div class="author">
                  <a style="font-size: 12pt" href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>: <span class="me"><u>Xian Liu</u> (Core Contributor)</span>.
                </div>
                <div class="conf">
                  Contributions: Auto-Regressive Foundation Model Pre-Training & Post-Training. <font color="red"> (CES'25 Best of AI, Best Overall)</font>
                </div>
                <div>
                  <span class="tag"> <a href="https://www.nvidia.com/en-us/ai/cosmos/" target="_blank">Webpage</a> </span> /
                  <span class="tag"> <a href="https://research.nvidia.com/labs/dir/cosmos1/" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2501.03575" target="_blank">Paper</a> </span> /
                  <span class="tag"> <a href="https://blogs.nvidia.com/blog/cosmos-world-foundation-models/" target="_blank">Blog</a> </span> /
                  <span class="tag"> <a href="https://github.com/NVIDIA/Cosmos" target="_blank">Github</a> </span> /
                  <span class="tag"> <a href="https://huggingface.co/collections/nvidia/cosmos-6751e884dc10e013a0a0d8e6" target="_blank">HuggingFace</a> </span> /
                  <span class="tag"> <a href="https://www.youtube.com/watch?v=9Uch931cDx8" target="_blank">Demo</a> </span> /
                  <span class="tag"> <a href="https://build.nvidia.com/explore/discover" target="_blank">Preview</a> </span> /
                  <span class="tag"> <a href="https://www.youtube.com/live/k82RwXqZHY8?t=3303s" target="_blank">Keynote (Jensen Huang, CES'25)</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="80" style="padding: 10px 0px 10px 0px; margin-right: 25px;" src="./images/tokenizer.jpg">
              </td>
              <td>
                <div class="title">
                  Cosmos Tokenizer: A Suite of Image and Video Neural Tokenizers
                </div>
                <div class="author">
                  <a style="font-size: 12pt" href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>: <span class="me"><u>Xian Liu</u> (Core Contributor)</span>.
                </div>
                <div class="conf">
                  Contributions: Continuous/Discrete Image/Video Tokenizers.
                </div>
                <div>
                  <span class="tag"> <a href="https://research.nvidia.com/labs/dir/cosmos-tokenizer" target="_blank">Webpage</a> </span> /
                  <span class="tag"> <a href="https://research.nvidia.com/labs/dir/cosmos-tokenizer" target="_blank">Technical Report</a> </span> /
                  <span class="tag"> <a href="https://developer.nvidia.com/blog/state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo/" target="_blank">Blog</a> </span> /
                  <span class="tag"> <a href="https://github.com/NVIDIA/Cosmos-Tokenizer" target="_blank">Github</a> </span> /
                  <span class="tag"> <a href="https://huggingface.co/collections/nvidia/cosmos-tokenizer-672b93023add81b66a8ff8e6" target="_blank">HuggingFace</a> </span> /
                  <span class="tag"> <a href="https://youtu.be/Soy_myOfWIU" target="_blank">Demo</a> </span> /
                  <span class="tag"> <a href="https://github.com/NVlabs/TokenBench" target="_blank">Benchmark</a> </span>
                </div>
              </td>
            </tr>
          </tbody>        
          </table>
        </p>

        <!-- <h2>Selected Preprints</h2>
        <p style="margin:-20pt 0px">
        <table style="border-collapse:separate; border-spacing:0px 35px;" cellspacing="8">
          <tbody>

          </tbody>        
          </table>
        </p> -->

        <h2>Selected Publications [<a style="font-size: 15pt" href="./pub.html"> Full List </a>] <font color="black" size="2">(* indicates equal contribution)</font> </h2>
        <p style="margin:-20pt 0px">
          <table style="border-collapse:separate; border-spacing:0px 35px;" cellspacing="8">
          <tbody>

            <tr>
              <td width="23%">
                <img width="200" height="105" style="padding: 10px 0px 10px 0px" src="./images/hmar.png">
              </td>
              <td>
                <div class="title">
                  HMAR: Efficient Hierarchical Masked AutoRegressive Image Generation
                </div>
                <div class="author">
                  <a href="https://scholar.google.com/citations?user=mieuBzUAAAAJ&hl=en">Hermann Kumbong*</a>,
                  <span class="me"><u>Xian Liu*</u></span>,
                  <a href="https://tsungyilin.info/">Tsung-Yi Lin</a>,
                  <a href="https://xh-liu.github.io/">Xihui Liu</a>,
                  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>,
                  <a href="http://www.danfu.org/">Daniel Y. Fu</a>,
                  <a href="http://mingyuliu.net/">Ming-Yu Liu</a>,
                  <a href="https://cs.stanford.edu/~chrismre/">Christopher RÃ©</a>,
                  <a href="https://davidwromero.xyz/">David W. Romero</a>.
                </div>
                <div class="conf">
                  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025.
                </div>
                <div>
                  <span class="venue"> <a href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a> </span> /
                  <span class="tag"> <a href="https://alvinliu0.github.io/" target="_blank">Paper (Coming Soon)</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="85" style="padding: 20px 0px 20px 0px" src="./images/t2vcompbench.png">
              </td>
              <td>
                <div class="title">
                  T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation
                </div>
                <div class="author">
                  <a href="https://scholar.google.com/citations?user=mieuBzUAAAAJ&hl=en">Kaiyue Sun</a>,
                  <a href="https://openreview.net/profile?id=~Kaiyi_Huang1">Kaiyi Huang</a>,
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://yuewuhkust.github.io/">Yue Wu</a>,
                  Zihan Xu,
                  <a href="http://www.ee.columbia.edu/~zgli/">Zhenguo Li</a>,
                  <a href="https://xh-liu.github.io/">Xihui Liu</a>.
                </div>
                <div class="conf">
                  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025.
                </div>
                <div>
                  <span class="venue"> <a href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a> </span> /
                  <span class="tag"> <a href="https://huggingface.co/spaces/Kaiyue/T2V-CompBench_Leaderboard" target="_blank">HuggingFace Leaderboard</a> </span> /
                  <span class="tag"> <a href="https://t2v-compbench.github.io/" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://github.com/KaiyueSun98/T2V-CompBench" target="_blank">Code</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/pdf/2407.14505" target="_blank">arXiv</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="125" style="padding: 0px 0px 0px 0px" src="./images/3dtrajmaster.gif">
              </td>
              <td>
                <div class="title">
                  3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation
                </div>
                <div class="author">
                  <a href="https://fuxiao0719.github.io/">Xiao Fu</a>,
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://xinntao.github.io/">Xintao Wang</a>,
                  <a href="https://pengsida.net/">Sida Peng</a>,
                  <a href="https://menghanxia.github.io/">Menghan Xia</a>,
                  <a href="https://xiaoyushi97.github.io/">Xiaoyu Shi</a>,
                  <a href="https://scholar.google.ru/citations?user=fWxWEzsAAAAJ&hl=en">Ziyang Yuan</a>,
                  <a href="https://scholar.google.com/citations?user=P6MraaYAAAAJ&hl=en">Pengfei Wan</a>,
                  <a href="https://openreview.net/profile?id=~Di_ZHANG3">Di Zhang</a>,
                  <a href="http://dahua.site/">Dahua Lin</a>.
                </div>
                <div class="conf">
                  International Conference on Learning Representations (<b>ICLR</b>), 2025.
                </div>
                <div>
                  <span class="venue"> <a href="https://iclr.cc/Conferences/2025">ICLR 2025</a> </span> /
                  <span class="tag"> <a href="https://openreview.net/forum?id=Gx04TnVjee" target="_blank">OpenReview</a> </span> /
                  <span class="tag"> <a href="http://fuxiao0719.github.io/projects/3dtrajmaster" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://github.com/kwaiVGI/3DTrajMaster" target="_blank">Code</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2412.07759" target="_blank">arXiv</a> </span> /
                  <span class="tag"> <a href="https://huggingface.co/datasets/KwaiVGI/360Motion-Dataset" target="_blank">Dataset</a>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="125" style="padding: 0px 0px 0px 0px" src="./images/videovae.png">
              </td>
              <td>
                <div class="title">
                  High-Quality Joint Image and Video Tokenization with Causal VAE
                </div>
                <div class="author">
                  <a href="https://dawitmureja.github.io/">Dawit Mureja Argaw</a>,
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://qsh-zh.github.io/">Qinsheng Zhang</a>,
                  <a href="https://scholar.google.co.uk/citations?user=JJ_LQ0YAAAAJ&hl=en">Joon Son Chung</a>,
                  <a href="http://mingyuliu.net/">Ming-Yu Liu</a>,
                  <a href="https://fitsumreda.github.io/">Fitsum Reda</a>.
                </div>
                <div class="conf">
                  International Conference on Learning Representations (<b>ICLR</b>), 2025.
                </div>
                <div>
                  <span class="venue"> <a href="https://iclr.cc/Conferences/2025">ICLR 2025</a> </span> /
                  <span class="tag"> <a href="https://openreview.net/forum?id=aRD1NqcXTC" target="_blank">OpenReview</a> </span>
                </div>
              </td>
            </tr>
            
            <tr>
              <td width="23%">
                <img width="200" height="125" style="padding: 0px 0px 0px 0px" src="./images/sjd.png">
              </td>
              <td>
                <div class="title">
                  Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding
                </div>
                <div class="author">
                  <a href="https://tyshiwo1.github.io/">Yao Teng</a>,
                  Han Shi,
                  <span class="me"><u>Xian Liu</u></span>,
                  Xuefei Ning,
                  Guohao Dai,
                  Yu Wang,
                  Zhenguo Li,
                  <a href="https://xh-liu.github.io/">Xihui Liu</a>.
                </div>
                <div class="conf">
                  International Conference on Learning Representations (<b>ICLR</b>), 2025.
                </div>
                <div>
                  <span class="venue"> <a href="https://iclr.cc/Conferences/2025">ICLR 2025</a> </span> /
                  <span class="tag"> <a href="https://openreview.net/forum?id=LZfjxvqw0N" target="_blank">OpenReview</a> </span> /
                  <span class="tag"> <a href="https://github.com/tyshiwo1/Accelerating-T2I-AR-with-SJD" target="_blank">Code</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2410.01699" target="_blank">arXiv</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="180" height="125" style="padding: 0px 10px 0px 10px" src="./images/edgerunner.png">
              </td>
              <td>
                <div class="title">
                  EdgeRunner: Auto-regressive Auto-encoder for Artistic Mesh Generation
                </div>
                <div class="author">
                  <a href="https://me.kiui.moe/">Jiaxiang Tang</a>,
                  <a href="https://mli0603.github.io/">Zhaoshuo Li</a>,
                  <a href="https://zekunhao.com/">Zekun Hao</a>,
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://scholar.google.com/citations?user=RuHyY6gAAAAJ">Gang Zeng</a>,
                  <a href="https://mingyuliu.net/">Ming-Yu Liu</a>,
                  <a href="https://qsh-zh.github.io/">Qinsheng Zhang</a>.
                </div>
                <div class="conf">
                  International Conference on Learning Representations (<b>ICLR</b>), 2025.
                </div>
                <div>
                  <span class="venue"> <a href="https://iclr.cc/Conferences/2025">ICLR 2025</a> </span> /
                  <span class="tag"> <a href="https://openreview.net/forum?id=81cta3WQVI" target="_blank">OpenReview</a> </span> /
                  <span class="tag"> <a href="https://research.nvidia.com/labs/dir/edgerunner/" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://github.com/NVlabs/EdgeRunner" target="_blank">Code</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2409.18114" target="_blank">arXiv</a> </span> /
                  <span class="tag"> <a href="https://github.com/NVlabs/EdgeRunner/tree/main/meto" target="_blank">Mesh Tokenizer</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="180" height="125" style="padding: 0px 10px 0px 10px" src="./images/humangaussian-teaser.gif">
              </td>
              <td>
                <div class="title">
                  HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting
                </div>
                <div class="author">
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://xiaohangzhan.github.io/">Xiaohang Zhan</a>,
                  <a href="https://me.kiui.moe/">Jiaxiang Tang</a>,
                  <a href='https://scholar.google.com/citations?user=4oXBp9UAAAAJ'>Ying Shan</a>,
                  <a href="https://scholar.google.com/citations?user=RuHyY6gAAAAJ">Gang Zeng</a>,
                  <a href="http://dahua.site/">Dahua Lin</a>,
                  <a href="https://xh-liu.github.io/">Xihui Liu</a>,
                  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>.
                </div>
                <div class="conf">
                  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <font color="red"> (Highlight, Top 2.8%)</font>
                </div>
                <div>
                  <span class="venue"> <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a> </span> /
                  <span class="tag"> <a href="https://alvinliu0.github.io/projects/HumanGaussian" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://alvinliu0.github.io/projects/HumanGaussian/humangaussian.pdf" target="_blank">Paper</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2311.17061" target="_blank">arXiv</a> </span> /
                  <span class="tag"> <a href="https://www.youtube.com/watch?v=S3djzHoqPKY" target="_blank">Demo Video</a> </span> /
                  <span class="tag"> <a href="https://github.com/alvinliu0/HumanGaussian" target="_blank">Code</a> </span> /
                  <span class="tag"> <a href="https://github.com/alvinliu0/HumanGaussian#pretrained-models" target="_blank">Models</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="180" height="125" style="padding: 0px 10px 0px 10px" src="./images/hyperhuman.png">
              </td>
              <td>
                <div class="title">
                  HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion
                </div>
                <div class="author">
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://alanspike.github.io/">Jian Ren</a>,
                  <a href="https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/">Aliaksandr Siarohin</a>,
                  <a href='https://universome.github.io/'>Ivan Skorokhodov</a>,
                  <a href="https://scholar.google.com/citations?user=XUj8koUAAAAJ&hl=en">Yanyu Li</a>,
                  <a href="http://dahua.site/">Dahua Lin</a>,
                  <a href="https://xh-liu.github.io/">Xihui Liu</a>,
                  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>,
                  <a href="http://www.stulyakov.com/">Sergey Tulyakov</a>.
                </div>
                <div class="conf">
                  International Conference on Learning Representations (<b>ICLR</b>), 2024. <font color="red"> (Review Score 6, 6, 8, 10, Top 1.6%, <a style="font-size: 12pt" href="https://papercopilot.com/statistics/iclr-statistics/iclr-2024-statistics/">Rank</a>)</font>
                </div>
                <div>
                  <span class="venue"> <a href="https://iclr.cc/Conferences/2024">ICLR 2024</a> </span> /
                  <span class="tag"> <a href="https://openreview.net/forum?id=duyA42HlCK" target="_blank">OpenReview</a> </span> /
                  <span class="tag"> <a href="https://snap-research.github.io/HyperHuman/" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://snap-research.github.io/HyperHuman/content/hyperhuman.pdf" target="_blank">Paper</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2310.08579" target="_blank">arXiv</a> </span> /
                  <span class="tag"> <a href="https://www.youtube.com/watch?v=eRPZW1pwxog" target="_blank">Short Demo (3min)</a> </span> /
                  <span class="tag"> <a href="https://www.youtube.com/watch?v=CxGfbwZOcyU" target="_blank">Long Demo (10min)</a> </span> /
                  <span class="tag"> <a href="https://github.com/snap-research/HyperHuman" target="_blank">Github</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="100" style="padding: 12.5px 0px 12.5px 0px" src="./images/SSP-NeRF.png">
              </td>
              <td>
                <div class="title">
                  Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation
                </div>
                <div class="author">
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://justimyhxu.github.io/">Yinghao Xu</a>,
                  <a href="https://wuqianyi.top/">Qianyi Wu</a>,
                  <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>,
                  <a href="https://wywu.github.io/">Wayne Wu</a>,
                  <a href="http://bzhou.ie.cuhk.edu.hk/">Bolei Zhou</a>.
                </div>
                <div class="conf">
                  European Conference on Computer Vision (<b>ECCV</b>), 2022. <font color="red"> (Oral, Top 2.7%)</font>
                </div>
                <div>
                  <span class="venue"> <a href="https://eccv2022.ecva.net/">ECCV 2022</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/pdf/2201.07786.pdf" target="_blank">Paper</a> </span> /
                  <span class="tag"> <a href="https://alvinliu0.github.io/projects/SSP-NeRF" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://github.com/alvinliu0/SSP-NeRF" target="_blank">Code</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="100" style="padding: 12.5px 0px 12.5px 0px" src="./images/angie.png">
              </td>
              <td>
                <div class="title">
                  Audio-Driven Co-Speech Gesture Video Generation
                </div>
                <div class="author">
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://wuqianyi.top/">Qianyi Wu</a>,
                  <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>,
                  <a href='https://yuanqidu.github.io/'>Yuanqi Du</a>,
                  <a href="https://wywu.github.io/">Wayne Wu</a>,
                  <a href="http://dahua.site/">Dahua Lin</a>,
                  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>.
                </div>
                <div class="conf">
                  Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2022. <font color="red"> (Spotlight, Top 5%)</font>
                </div>
                <div>
                  <span class="venue"> <a href="https://neurips.cc/">NeurIPS 2022</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/pdf/2212.02350.pdf" target="_blank">Paper</a> </span> /
                  <span class="tag"> <a href="https://alvinliu0.github.io/projects/ANGIE" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://github.com/alvinliu0/ANGIE" target="_blank">Code</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="100" style="padding: 12.5px 0px 12.5px 0px" src="./images/HA2G.png">
              </td>
              <td>
                <div class="title">
                  Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation
                </div>
                <div class="author">
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://wuqianyi.top/">Qianyi Wu</a>,
                  <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>,
                  <a href="https://justimyhxu.github.io/">Yinghao Xu</a>,
                  <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>,
                  <a href="https://alvinliu0.github.io/">Xinyi Lin</a>,
                  <a href="https://xzhou.me/">Xiaowei Zhou</a>,
                  <a href="https://wywu.github.io/">Wayne Wu</a>,
                  <a href="http://daibo.info/">Bo Dai</a>,
                  <a href="http://bzhou.ie.cuhk.edu.hk/">Bolei Zhou</a>.
                </div>
                <div class="conf">
                  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022.
                </div>
                <div class="conf">
                  Also appears at CVPR 2022 <a href="https://sightsound.org/">Sight and Sound Workshop</a>. [5-min Invited Talk] <a href="https://drive.google.com/file/d/1oAJ863xFtY4ycZmZOtFD_0DYHfCH8w6T/view?usp=sharing">(link)</a> 
                </div>
                <div>
                  <span class="venue"> <a href="http://cvpr2022.thecvf.com/">CVPR 2022</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/pdf/2203.13161.pdf">Paper</a> </span> /
                  <span class="tag"> <a href="https://alvinliu0.github.io/projects/HA2G/HA2G_poster.pdf">Poster</a> </span> /
                  <span class="tag"> <a href="https://alvinliu0.github.io/projects/HA2G">Project</a> </span> /
                  <span class="tag"> <a href="https://github.com/alvinliu0/HA2G">Code</a> </span>
              </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="80" style="padding: 22.5px 0px 22.5px 0px" src="./images/diffgesture.png">
              </td>
              <td>
                <div class="title">
                  Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation
                </div>
                <div class="author">
                  <span class="me"><u>Xian Liu*</u></span>,
                  <a href="https://scholar.google.com/citations?user=TPD_P98AAAAJ&hl=zh-CN">Lingting Zhu*</a>,
                  Xuanyu Liu,
                  <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>,
                  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>,
                  <a href="https://yulequan.github.io/">Lequan Yu</a>.
                </div>
                <div class="conf">
                  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023.
                </div>
                <div>
                  <span class="venue"> <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/pdf/2303.09119.pdf" target="_blank">Paper</a> </span> /
                  <span class="tag"> <a href="https://github.com/Advocate99/DiffGesture" target="_blank">Code</a> </span>
                </div>
              </td>
            </tr>
            
            <tr>
              <td width="23%">
                <img width="200" height="125" src="images/ier.png" border="0">
                      </td>
              <td>
              <div class="title">Visual Sound Localization in the Wild by Cross-Modal Interference Erasing</div>
            
              <div class="author">
                  <span class="me"><u>Xian Liu*</u></span>, 
                  <a href='https://shvdiwnkozbw.github.io/'>Rui Qian*</a>, 
                  <a href='https://hangz-nju-cuhk.github.io/'>Hang Zhou*</a>, 
                  <a href="https://dtaoo.github.io/">Di Hu</a>, 
                  <a href="https://weiyaolin.github.io/">Weiyao Lin</a>,
                  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>, 
                  <a href="http://bzhou.ie.cuhk.edu.hk/">Bolei Zhou</a>, 
                  <a href="https://xzhou.me/">Xiaowei Zhou</a>.
              </div>
              <div class="conf">
                AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2022.
              </div>
              <div>
                  <span class="venue"> <a href="https://aaai.org/Conferences/AAAI-22/">AAAI 2022</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/pdf/2202.06406.pdf">Paper</a> </span> /
                  <span class="tag"> <a href="https://alvinliu0.github.io/projects/ier_poster.pdf">Poster</a> </span>
              </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="125" src="./images/tc4d.gif">
              </td>
              <td>
                <div class="title">
                  TC4D: Trajectory-Conditioned Text-to-4D Generation
                </div>
                <div class="author">
                  <a href="https://sherwinbahmani.github.io/">Sherwin Bahmani*</a>,
                  <span class="me"><u>Xian Liu*</u></span>,
                  <a href="https://yifita.netlify.app/">Yifan Wang*</a>,
                  <a href="https://universome.github.io/">Ivan Skorokhodov</a>,
                  <a href="https://www.lessvrong.com/">Victor Rong</a>,
                  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>,
                  <a href="https://xh-liu.github.io/">Xihui Liu</a>,
                  <a href="https://jjparkcv.github.io/">Jeong Joon Park</a>,
                  <a href="http://www.stulyakov.com/">Sergey Tulyakov</a>,
                  <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
                  <a href="https://taiya.github.io/">Andrea Tagliasacchi</a>,
                  <a href="https://davidlindell.com/">David B. Lindell</a>.
                </div>
                <div class="conf">
                  European Conference on Computer Vision (<b>ECCV</b>), 2024.
                </div>
                <div>
                  <span class="venue"> <a href="https://eccv2024.ecva.net/">ECCV 2024</a> </span> /
                  <span class="tag"> <a href="https://sherwinbahmani.github.io/tc4d/" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2403.17920" target="_blank">arXiv</a> </span> /
                  <span class="tag"> <a href="https://github.com/sherwinbahmani/tc4d" target="_blank">Github</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="125" src="./images/objsdf.png">
              </td>
              <td>
                <div class="title">
                  Object-Compositional Neural Implicit Surfaces
                </div>
                <div class="author">
                  <a href="https://wuqianyi.top/">Qianyi Wu</a>,
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://donydchen.github.io/">Yuedong Chen</a>,
                  <a href="https://likojack.github.io/kejieli/#/home">Kejie Li</a>,
                  <a href="https://www.chuanxiaz.com/">Chuanxia Zheng</a>,
                  <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
                  <a href="http://www.ntu.edu.sg/home/asjmzheng/">Jianmin Zheng</a>.
                </div>
                <div class="conf">
                  European Conference on Computer Vision (<b>ECCV</b>), 2022.
                </div>
                <div>
                  <div>
                    <span class="venue"> <a href="https://eccv2022.ecva.net/">ECCV 2022</a> </span> /
                    <span class="tag"> <a href="http://arxiv.org/pdf/2207.09686.pdf" target="_blank">Paper</a> </span> /
                    <span class="tag"> <a href="https://qianyiwu.github.io/objectsdf/" target="_blank">Project</a> </span> /
                    <span class="tag"> <a href="https://github.com/QianyiWu/objsdf" target="_blank">Code</a> </span>
                  </div>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="200" height="125" src="./images/brushnet.png">
              </td>
              <td>
                <div class="title">
                  BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion
                </div>
                <div class="author">
                  <a href="https://juxuan27.github.io/">Xuan Ju</a>,
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://xinntao.github.io/">Xintao Wang</a>,
                  <a href="https://scholar.google.com/citations?hl=en-us&user=HzemVzoAAAAJ">Yuxuan Bian</a>,
                  <a href='https://scholar.google.com/citations?user=4oXBp9UAAAAJ'>Ying Shan</a>,
                  <a href="https://www.cse.cuhk.edu.hk/~qxu/">Qiang Xu</a>.
                </div>
                <div class="conf">
                  European Conference on Computer Vision (<b>ECCV</b>), 2024.
                </div>
                <div>
                  <span class="venue"> <a href="https://eccv2024.ecva.net/">ECCV 2024</a> </span> /
                  <span class="tag"> <a href="https://tencentarc.github.io/BrushNet/" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2403.06976" target="_blank">arXiv</a> </span> /
                  <span class="tag"> <a href="https://drive.google.com/file/d/1IkEBWcd2Fui2WHcckap4QFPcCI0gkHBh/view" target="_blank">Demo Video</a> </span> /
                  <span class="tag"> <a href="https://forms.gle/9TgMZ8tm49UYsZ9s5" target="_blank">Dataset</a> </span> /
                  <span class="tag"> <a href="https://github.com/TencentARC/BrushNet" target="_blank">Github</a> </span> /
                  <span class="tag"> <a href="https://huggingface.co/spaces/TencentARC/BrushNet" target="_blank">HuggingFace Demo</a> </span>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%" valign="middle">
                <img width="200" height="125" src="./images/textcrafter.png">
              </td>
              <td>
                <div class="title">
                  TextCraftor: Your Text Encoder Can be Image Quality Controller
                </div>
                <div class="author">
                  <a href="https://scholar.google.com/citations?user=XUj8koUAAAAJ&hl=en">Yanyu Li</a>,
                  <span class="me"><u>Xian Liu</u></span>,
                  <a href="https://anilkagak2.github.io/">Anil Kag</a>,
                  <a href="https://www.linkedin.com/in/erichuju">Ju Hu</a>,
                  <a href='https://scholar.google.com/citations?user=nAaroNMAAAAJ'>Yerlan Idelbayev</a>,
                  <a href="https://www.linkedin.com/in/dhritiman-sagar-5775169/">Dhritiman Sagar</a>,
                  <a href="https://web.northeastern.edu/yanzhiwang/">Yanzhi Wang</a>,
                  <a href="http://www.stulyakov.com/">Sergey Tulyakov</a>,
                  <a href="https://alanspike.github.io/">Jian Ren</a>.
                </div>
                <div class="conf">
                  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.
                </div>
                <div>
                  <span class="venue"> <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a> </span> /
                  <span class="tag"> <a href="https://snap-research.github.io/textcraftor/" target="_blank">Project</a> </span> /
                  <span class="tag"> <a href="https://github.com/snap-research/textcraftor" target="_blank">Code</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/pdf/2403.18978.pdf" target="_blank">Paper</a> </span> /
                  <span class="tag"> <a href="https://arxiv.org/abs/2403.18978" target="_blank">arXiv</a> </span>
                </div>
              </td>
            </tr>

          </tbody>        
          </table>
        </p>

        <h2>Experiences</h2>
        <!-- <font face="helvetica, ariel, 'sans serif'"">      -->
        <!-- <ul style="font-size: 12pt; text-align: justify;"> -->
        <!-- <li><si> Research Intern, Intelligent Vision Group, Ant Research Institute, Alibaba.</si>
          <br> Feb. 2022 - Now
          <br> Supervised by: <a href="https://sites.google.com/site/zhaodeli/">Prof. Deli Zhao</a>.</li> -->
        
          <p style="margin:-20pt 0px">
          <table style="border-collapse:separate; border-spacing:10px 30px;" cellspacing="8">
          <tbody>

            <tr>
              <td width="23%">
                <img width="150" height="90" src="./images/nvidia.png">
              </td>
              <td>
                <div class="title">
                  Research Scientist.
                </div>
                <div class="author">
                  Jun. 2024 - Now
                </div>
                <div class="conf">
                  NVIDIA Research, Deep Imagination Research Group.
                </div>
                <div>
                  Manager: <a href="https://mingyuliu.net">Ming-Yu Liu</a>.
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="150" height="90" src="./images/nvidia.png">
              </td>
              <td>
                <div class="title">
                  Generative AI Research Intern, Deep Imagination Research, NVIDIA Research.
                </div>
                <div class="author">
                  Mar. 2024 - Jun. 2024
                </div>
                <div class="conf">
                  Topic: Image/Video Foundation Models, Tokenizers, Multi-Modal Language Models.
                </div>
                <div>
                  Supervised by: <a href="https://tcwang0509.github.io/">Ting-Chun Wang</a>, <a href="https://yogeshbalaji.github.io/">Yogesh Balaji</a>, <a href="https://mingyuliu.net">Ming-Yu Liu</a>.
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="150" height="70" style="padding: 10px 0px 10px 0px" src="./images/UofT-logo.svg.png">
              </td>
              <td>
                <div class="title">
                  Research Visiting Student, Toronto Computational Imaging Group.
                </div>
                <div class="author">
                  Dec. 2023 - Mar. 2024
                </div>
                <div class="conf">
                  Topic: Text-to-4D Generation.
                </div>
                <div>
                  Hosted by: <a href="https://sherwinbahmani.github.io/">Sherwin Bahmani</a>, <a href="https://davidlindell.com/">David B. Lindell</a>.
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="150" height="90" src="./images/txailab.jpeg">
              </td>
              <td>
                <div class="title">
                  Research Intern, Tencent AI Laboratory.
                </div>
                <div class="author">
                  Sept. 2023 - Dec. 2023
                </div>
                <div class="conf">
                  Topic: Text-Driven 3D Human Generation.
                </div>
                <div>
                  Supervised by: <a href="https://xiaohangzhan.github.io/">Xiaohang Zhan</a>, <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ">Ying Shan</a>.
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="90" height="90" style="padding: 0px 30px 0px 30px" src="./images/snap.png">
              </td>
              <td>
                <div class="title">
                  Research Intern, Creative Vision Group, Snap Research.
                </div>
                <div class="author">
                  May. 2023 - Sept. 2023
                </div>
                <div class="conf">
                  Topic: Human Generation Foundation Model.
                </div>
                <div>
                  Supervised by: <a href="https://alanspike.github.io/">Jian Ren</a>, <a href="https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/">Aliaksandr Siarohin</a>, <a href="http://www.stulyakov.com/">Sergey Tulyakov</a>.
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="130" height="90" style="padding: 0px 10px 0px 10px" src="./images/shlab.png">
              </td>
              <td>
                <div class="title">
                  Research Intern, Digital Content Group, Shanghai AI Laboratory.
                </div>
                <div class="author">
                  Jul. 2021 - Feb. 2022
                </div>
                <div class="conf">
                  Topic: Digital Human, Gesture Generation.
                </div>
                <div>
                  Supervised by: <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>, <a href="https://wywu.github.io/">Wayne Wu</a>.</li>
                </div>
              </td>
            </tr>

            <tr>
              <td width="23%">
                <img width="150" height="40" style="padding: 25px 0px 25px 0px" src="./images/sensetime.png">
              </td>
              <td>
                <div class="title">
                  Research Intern, Intelligent Video Group, SenseTime Research.
                </div>
                <div class="author">
                  Aug. 2020 - Jun. 2021
                </div>
                <div class="conf">
                  Topic: Digital Human, Face Animation.
                </div>
                <div>
                  Supervised by: <a href="https://wuqianyi.top/">Qianyi Wu</a>, <a href="http://daibo.info/">Bo Dai</a>.</li>
                </div>
              </td>
            </tr>

          </tbody>        
          </table>
        </p>

        <h2>Invited Talks</h2>
        <!-- <font face="helvetica, ariel, 'sans serif'">  -->
        <ul style="font-size: 12pt; text-align: justify;">
        <li> AI TIME: <a href="https://aitime.cn/">HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion</a>. <div style="float:right; text-align:right">2024</div> </li>
        <li> AI TIME: <a href="https://aitime.cn/">Audio-Driven Co-Speech Gesture Video Generation</a>. <div style="float:right; text-align:right">2023</div> </li>
        <li> TechBeat: <a href="https://www.techbeat.net/">Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation</a>. <div style="float:right; text-align:right">2022</div> </li>
        </ul>

        <h2>Professional Services</h2>
        <!-- <font face="helvetica, ariel, 'sans serif'">  -->
          <ul style="font-size: 12pt; text-align: justify;">
          <li>Conference Program Committee / Reviewer: CVPR, ECCV, ICCV, SIGGRAPH, SIGGRAPH Asia, NeurIPS, ICML, ICLR, AISTATS, AAAI.</li>
          <li>Journal Reviewer: TPAMI, IJCV, TVCG, EG, CGF, PG.</li>
        </ul>
        <!-- </font> -->


        <h2>Selected Honors & Awards</h2>
        <!-- <font face="helvetica, ariel, 'sans serif'">  -->
        <ul style="font-size: 12pt; text-align: justify;">
        <li> CNET 2025 Best of CES, Best of AI, and Best Overall. <div style="float:right; text-align:right">2025</div> </li>
        <li> ECCV Outstanding Reviewer Award. <div style="float:right; text-align:right">2024</div> </li>
        <li> CVPR Travel Award. <div style="float:right; text-align:right">2024</div> </li>
        <li> ICLR Travel Award. <div style="float:right; text-align:right">2024</div> </li>
        <li> National Scholarship. <div style="float:right; text-align:right">2019, 2020</div> </li>
        <li> Hong Kong Ph.D. Fellowship Scheme (HKPFS). <div style="float:right; text-align:right">2021- 2025</div> </li>
        <li> Outstanding Graduate of Zhejiang Province.  <div style="float:right; text-align:right">2021</div> </li>
        <li> Outstanding Bachelor Thesis Award of Zhejiang University, Top 1%.  <div style="float:right; text-align:right">2021</div> </li>
        <li> UCLA CSST Scholarship Program.  <div style="float:right; text-align:right">2020</div> </li>
        <li> SenseTime Scholarship.  <div style="float:right; text-align:right">2020</div> </li>
        <li> Tang Lixin Scholarship.  <div style="float:right; text-align:right">2019</div> </li>
        <li> First Class Scholarship for Academic Excellence.  <div style="float:right; text-align:right">2019, 2020</div> </li>
        </ul>
        <!-- </font> -->

        <h2>Teaching Experience</h2>
        <!-- <font face="helvetica, ariel, 'sans serif'">  -->
          <ul style="font-size: 12pt; text-align: justify;">
          <!-- Teaching Assistant of the following courses in The Chinese University of Hong Kong:  -->
          <li>ENGG 1120, Linear Algebra for Engineers. <div style="float:right; text-align:right">Spring 2022.</div></li>
          <li>ENGG 2440, Discrete Mathematics for Engineers. <div style="float:right; text-align:right">Fall 2021.</div></li>
        </ul>
        <!-- </font> -->
      
        </body>


</html>
